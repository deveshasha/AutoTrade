{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forex Predictor LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time      245444\n",
      "Open      245444\n",
      "High      245444\n",
      "Low       245444\n",
      "Close     245444\n",
      "Volume    245444\n",
      "dtype: int64\n",
      "                        open     high      low    close       volume  \\\n",
      "timestamp                                                              \n",
      "2010-01-01 00:00:00  1.43283  1.43293  1.43224  1.43293  608600007.1   \n",
      "2010-01-01 00:15:00  1.43285  1.43295  1.43229  1.43275  535600003.2   \n",
      "2010-01-01 00:30:00  1.43280  1.43303  1.43239  1.43281  436299999.2   \n",
      "2010-01-01 00:45:00  1.43285  1.43294  1.43229  1.43276  614299997.3   \n",
      "2010-01-01 01:00:00  1.43287  1.43292  1.43206  1.43282  705300008.8   \n",
      "\n",
      "                         momentum  avg_price  ohlc_price  oc_diff  \n",
      "timestamp                                                          \n",
      "2010-01-01 00:00:00 -60860.000710   1.432585    1.432732 -0.00010  \n",
      "2010-01-01 00:15:00  53560.000320   1.432620    1.432710  0.00010  \n",
      "2010-01-01 00:30:00  -4362.999992   1.432710    1.432758 -0.00001  \n",
      "2010-01-01 00:45:00  55286.999757   1.432615    1.432710  0.00009  \n",
      "2010-01-01 01:00:00  35265.000440   1.432490    1.432667  0.00005  \n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from config import *\n",
    "\n",
    "df = pd.read_csv(\"EURUSD_15m_BID_01.01.2010-31.12.2016.csv\")\n",
    "print(df.count())\n",
    "\n",
    "# Rename bid OHLC columns\n",
    "df.rename(columns={'Time': 'timestamp', 'Open': 'open', 'Close': 'close',\n",
    "                   'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)\n",
    "df.set_index('timestamp', inplace=True)\n",
    "df = df.astype(float)\n",
    "\n",
    "# Add additional features\n",
    "df['momentum'] = df['volume'] * (df['open'] - df['close'])\n",
    "df['avg_price'] = (df['low'] + df['high']) / 2\n",
    "# df['range'] = df['high'] - df['low']\n",
    "df['ohlc_price'] = (df['low'] + df['high'] + df['open'] + df['close']) / 4\n",
    "df['oc_diff'] = df['open'] - df['close']\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=20):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "# Scale and create datasets\n",
    "target_index = df.columns.tolist().index('close')\n",
    "high_index = df.columns.tolist().index('high')\n",
    "low_index = df.columns.tolist().index('low')\n",
    "dataset = df.values.astype('float32')\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Create y_scaler to inverse it later\n",
    "y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "t_y = df['close'].values.astype('float32')\n",
    "t_y = np.reshape(t_y, (-1, 1))\n",
    "y_scaler = y_scaler.fit(t_y)\n",
    "\n",
    "X, y = create_dataset(dataset, look_back=50)\n",
    "y = y[:, target_index]\n",
    "\n",
    "train_size = int(len(X) * 0.99)\n",
    "trainX = X[:train_size]\n",
    "trainY = y[:train_size]\n",
    "testX = X[train_size:]\n",
    "testY = y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 50, 90)            72000     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50, 30)            14520     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50, 20)            4080      \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 50, 10)            1240      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 10)            0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 4)                 240       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 92,105\n",
      "Trainable params: 92,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Bidirectional(LSTM(90, input_shape=(X.shape[1], X.shape[2]),\n",
    "                       return_sequences=True),\n",
    "                  merge_mode='sum',\n",
    "                  weights=None,\n",
    "                  input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(LSTM(30, return_sequences=True))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(LSTM(10, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(4, return_sequences=False))\n",
    "model.add(Dense(4, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(bi_rnn_weights,\n",
    "                                 monitor='val_mean_squared_error',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 218645 samples, validate on 24294 samples\n",
      "Epoch 1/10\n",
      "218645/218645 [==============================] - 299s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 0.03145, saving model to bi_rnn_trained_models/weights.best.hdf5\n",
      "Epoch 2/10\n",
      "218645/218645 [==============================] - 308s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00002: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 3/10\n",
      "218645/218645 [==============================] - 284s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 4/10\n",
      "218645/218645 [==============================] - 283s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 5/10\n",
      "218645/218645 [==============================] - 283s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00005: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 6/10\n",
      "218645/218645 [==============================] - 285s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 7/10\n",
      "218645/218645 [==============================] - 286s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00007: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 8/10\n",
      "218645/218645 [==============================] - 284s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 9/10\n",
      "218645/218645 [==============================] - 283s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 0.03145\n",
      "Epoch 10/10\n",
      "218645/218645 [==============================] - 283s 1ms/step - loss: 0.3552 - mean_absolute_error: 0.5514 - mean_squared_error: 0.3552 - val_loss: 0.0315 - val_mean_absolute_error: 0.1714 - val_mean_squared_error: 0.0315\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve from 0.03145\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainX, trainY, epochs=10, batch_size=1024, verbose=1, callbacks=callbacks_list,\n",
    "                        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(bi_rnn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testX)\n",
    "pred = y_scaler.inverse_transform(pred)\n",
    "close = y_scaler.inverse_transform(np.reshape(testY, (testY.shape[0], 1)))\n",
    "predictions = pd.DataFrame()\n",
    "predictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\n",
    "predictions['close'] = pd.Series(np.reshape(close, (close.shape[0])))\n",
    "predictions['diff'] = predictions['predicted'] - predictions['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df[-pred.shape[0]:].copy()\n",
    "predictions.index = p.index\n",
    "predictions = predictions.astype(float)\n",
    "predictions = predictions.merge(p[['low', 'high']], right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = pd.DataFrame(predictions.to_records())\n",
    "flattened['forex_pair']=\"EURUSD\"\n",
    "flattened['timestamp']=flattened.timestamp.apply(lambda x: x.strftime('%Y%m%d %H:%M:%S'))\n",
    "flattened['decision']=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flattened.loc[flattened['diff'] > 0, 'decision'] = 'sell'\n",
    "flattened.loc[flattened['diff'] == 0, 'decision'] = 'hold'\n",
    "flattened.loc[flattened['diff'] < 0, 'decision'] = 'buy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=flattened[['forex_pair','timestamp','decision']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=result.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, json\n",
    "with io.open('predictions.json', 'w', encoding='utf-8') as f:\n",
    "  f.write(json.dumps(data, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
